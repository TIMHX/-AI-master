{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongxing/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function  \n",
    "import keras\n",
    "from keras.models import Sequential                \n",
    "from keras.layers import Dense, Activation         \n",
    "from keras.optimizers import SGD                    \n",
    "\n",
    "import pandas                                       \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas.read_csv(\"train.csv\") \n",
    "dataset = dataset.as_matrix() \n",
    "X,y = dataset[:,1:], dataset[:,0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 1, ..., 7, 6, 9]), array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y,X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of features is: (42000, 784)\n",
      "the shape of labels is: (42000,)\n",
      "the range of features is: 0 to 255\n",
      "the range of labels is: 0 to 9\n",
      "label 1 is 1\n",
      "label 2 is 0\n",
      "label 3 is 1\n",
      "label 4 is 4\n",
      "label 5 is 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105d2c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print ('the shape of features is:',X.shape)       #print data shape\n",
    "print ('the shape of labels is:',y.shape)       #print data shape\n",
    "\n",
    "import numpy as np\n",
    "print ('the range of features is:',np.min(X),'to',np.max(X))\n",
    "print ('the range of labels is:',np.min(y),'to',np.max(y))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "for i in range(5):\n",
    "    image = X[i]\n",
    "    plt.subplot(1,5, i+1)\n",
    "    image = image.reshape(28,28)\n",
    "    print ('label', i+1, 'is',y[i])\n",
    "    plt.imshow(image, cmap='Blues')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.20) # Split data\n",
    "\n",
    "x_train = x_train/255.0                           # normalize training data\n",
    "x_val = x_val/255.0                             # normalize testing data\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 532,490\n",
      "Trainable params: 532,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "batch_size = 256\n",
    "epochs = 300\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# build the model\n",
    "model = Sequential()                                         # define model to be sequential\n",
    "model.add(Dense(256, activation='relu',input_dim=784))       # first hidden layer with 256 neurons\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dense(256, activation='relu'))                     # second hidden layer with 256 neurons\n",
    "model.add(Dense(10, activation='softmax'))                   # output layer\n",
    "model.summary()                                              # print out summary for all layers \n",
    "\n",
    "\n",
    "my_optimizer = keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)                   # using learning rate 0.001\n",
    "model.compile(optimizer=my_optimizer,                        # using SGD with our set lr as optimizer\n",
    "              loss='categorical_crossentropy',               # using cross entropy loss\n",
    "              metrics=['accuracy'])                          # metric that is called during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/300\n",
      " - 2s - loss: 2.2987 - acc: 0.1398 - val_loss: 2.2605 - val_acc: 0.1902\n",
      "Epoch 2/300\n",
      " - 1s - loss: 1.9337 - acc: 0.3945 - val_loss: 1.4797 - val_acc: 0.4975\n",
      "Epoch 3/300\n",
      " - 1s - loss: 1.1836 - acc: 0.6198 - val_loss: 0.9535 - val_acc: 0.7221\n",
      "Epoch 4/300\n",
      " - 1s - loss: 0.8061 - acc: 0.7644 - val_loss: 0.7088 - val_acc: 0.7990\n",
      "Epoch 5/300\n",
      " - 1s - loss: 0.6217 - acc: 0.8271 - val_loss: 0.5720 - val_acc: 0.8402\n",
      "Epoch 6/300\n",
      " - 1s - loss: 0.5090 - acc: 0.8627 - val_loss: 0.4841 - val_acc: 0.8677\n",
      "Epoch 7/300\n",
      " - 1s - loss: 0.4390 - acc: 0.8815 - val_loss: 0.4353 - val_acc: 0.8773\n",
      "Epoch 8/300\n",
      " - 1s - loss: 0.3898 - acc: 0.8942 - val_loss: 0.3933 - val_acc: 0.8879\n",
      "Epoch 9/300\n",
      " - 1s - loss: 0.3576 - acc: 0.9024 - val_loss: 0.3697 - val_acc: 0.8944\n",
      "Epoch 10/300\n",
      " - 1s - loss: 0.3302 - acc: 0.9099 - val_loss: 0.3499 - val_acc: 0.9017\n",
      "Epoch 11/300\n",
      " - 1s - loss: 0.3090 - acc: 0.9146 - val_loss: 0.3260 - val_acc: 0.9073\n",
      "Epoch 12/300\n",
      " - 1s - loss: 0.2881 - acc: 0.9222 - val_loss: 0.3162 - val_acc: 0.9105\n",
      "Epoch 13/300\n",
      " - 1s - loss: 0.2718 - acc: 0.9270 - val_loss: 0.3038 - val_acc: 0.9121\n",
      "Epoch 14/300\n",
      " - 2s - loss: 0.2564 - acc: 0.9311 - val_loss: 0.2870 - val_acc: 0.9188\n",
      "Epoch 15/300\n",
      " - 1s - loss: 0.2426 - acc: 0.9348 - val_loss: 0.2784 - val_acc: 0.9205\n",
      "Epoch 16/300\n",
      " - 1s - loss: 0.2315 - acc: 0.9379 - val_loss: 0.2651 - val_acc: 0.9240\n",
      "Epoch 17/300\n",
      " - 1s - loss: 0.2186 - acc: 0.9412 - val_loss: 0.2565 - val_acc: 0.9276\n",
      "Epoch 18/300\n",
      " - 1s - loss: 0.2076 - acc: 0.9446 - val_loss: 0.2459 - val_acc: 0.9308\n",
      "Epoch 19/300\n",
      " - 1s - loss: 0.1993 - acc: 0.9457 - val_loss: 0.2377 - val_acc: 0.9349\n",
      "Epoch 20/300\n",
      " - 1s - loss: 0.1877 - acc: 0.9498 - val_loss: 0.2310 - val_acc: 0.9364\n",
      "Epoch 21/300\n",
      " - 2s - loss: 0.1790 - acc: 0.9519 - val_loss: 0.2304 - val_acc: 0.9352\n",
      "Epoch 22/300\n",
      " - 2s - loss: 0.1703 - acc: 0.9540 - val_loss: 0.2191 - val_acc: 0.9388\n",
      "Epoch 23/300\n",
      " - 2s - loss: 0.1619 - acc: 0.9564 - val_loss: 0.2115 - val_acc: 0.9411\n",
      "Epoch 24/300\n",
      " - 2s - loss: 0.1542 - acc: 0.9583 - val_loss: 0.2112 - val_acc: 0.9417\n",
      "Epoch 25/300\n",
      " - 2s - loss: 0.1475 - acc: 0.9603 - val_loss: 0.2047 - val_acc: 0.9431\n",
      "Epoch 26/300\n",
      " - 2s - loss: 0.1412 - acc: 0.9611 - val_loss: 0.1982 - val_acc: 0.9456\n",
      "Epoch 27/300\n",
      " - 2s - loss: 0.1331 - acc: 0.9636 - val_loss: 0.1904 - val_acc: 0.9470\n",
      "Epoch 28/300\n",
      " - 1s - loss: 0.1262 - acc: 0.9653 - val_loss: 0.1919 - val_acc: 0.9475\n",
      "Epoch 29/300\n",
      " - 2s - loss: 0.1200 - acc: 0.9674 - val_loss: 0.1865 - val_acc: 0.9488\n",
      "Epoch 30/300\n",
      " - 2s - loss: 0.1154 - acc: 0.9677 - val_loss: 0.1871 - val_acc: 0.9485\n",
      "Epoch 31/300\n",
      " - 2s - loss: 0.1094 - acc: 0.9703 - val_loss: 0.1821 - val_acc: 0.9506\n",
      "Epoch 32/300\n",
      " - 1s - loss: 0.1035 - acc: 0.9714 - val_loss: 0.1779 - val_acc: 0.9515\n",
      "Epoch 33/300\n",
      " - 1s - loss: 0.0983 - acc: 0.9740 - val_loss: 0.1784 - val_acc: 0.9508\n",
      "Epoch 34/300\n",
      " - 2s - loss: 0.0937 - acc: 0.9747 - val_loss: 0.1785 - val_acc: 0.9502\n",
      "Epoch 35/300\n",
      " - 2s - loss: 0.0891 - acc: 0.9758 - val_loss: 0.1734 - val_acc: 0.9525\n",
      "Epoch 36/300\n",
      " - 2s - loss: 0.0841 - acc: 0.9772 - val_loss: 0.1732 - val_acc: 0.9523\n",
      "Epoch 37/300\n",
      " - 2s - loss: 0.0802 - acc: 0.9788 - val_loss: 0.1714 - val_acc: 0.9545\n",
      "Epoch 38/300\n",
      " - 2s - loss: 0.0762 - acc: 0.9794 - val_loss: 0.1759 - val_acc: 0.9517\n",
      "Epoch 39/300\n",
      " - 2s - loss: 0.0727 - acc: 0.9809 - val_loss: 0.1704 - val_acc: 0.9536\n",
      "Epoch 40/300\n",
      " - 2s - loss: 0.0683 - acc: 0.9819 - val_loss: 0.1729 - val_acc: 0.9554\n",
      "Epoch 41/300\n",
      " - 2s - loss: 0.0667 - acc: 0.9813 - val_loss: 0.1701 - val_acc: 0.9544\n",
      "Epoch 42/300\n",
      " - 2s - loss: 0.0619 - acc: 0.9834 - val_loss: 0.1697 - val_acc: 0.9550\n",
      "Epoch 43/300\n",
      " - 2s - loss: 0.0585 - acc: 0.9847 - val_loss: 0.1677 - val_acc: 0.9569\n",
      "Epoch 44/300\n",
      " - 2s - loss: 0.0553 - acc: 0.9857 - val_loss: 0.1761 - val_acc: 0.9536\n",
      "Epoch 45/300\n",
      " - 2s - loss: 0.0529 - acc: 0.9860 - val_loss: 0.1745 - val_acc: 0.9554\n",
      "Epoch 46/300\n",
      " - 2s - loss: 0.0501 - acc: 0.9870 - val_loss: 0.1729 - val_acc: 0.9560\n",
      "Epoch 47/300\n",
      " - 2s - loss: 0.0476 - acc: 0.9880 - val_loss: 0.1749 - val_acc: 0.9542\n",
      "Epoch 48/300\n",
      " - 2s - loss: 0.0443 - acc: 0.9889 - val_loss: 0.1748 - val_acc: 0.9568\n"
     ]
    }
   ],
   "source": [
    "best_weights_filepath = './best_weights.hdf5' ##define the filename to store\n",
    "                                            ##the best performance and weights\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                                              patience = 5) \n",
    "#Stop training early if val_acc doesn't improve for 5 epochs\n",
    "\n",
    "SaveBestWeights = keras.callbacks.ModelCheckpoint(best_weights_filepath,\n",
    "                                                  monitor='val_acc',\n",
    "                                                  save_best_only=True)\n",
    "# store the historically best performing weights in best_weights_filepath\n",
    "#, where performance is given by accuracy on the validation set.\n",
    "\n",
    "\n",
    "model_history = model.fit(x_train, y_train,                   # training data \n",
    "                    batch_size=batch_size,                   # batch size 256\n",
    "                    epochs=epochs,                           # 15 epochs \n",
    "                    verbose= 2,                              # verbose level\n",
    "                    validation_data = (x_val, y_val),  #Use the previously defined x_test as a validation set. \n",
    "                    callbacks = [earlyStopping, SaveBestWeights]\n",
    "                         )     \n",
    "model.load_weights(best_weights_filepath) ##Set the best performing weights to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pandas.read_csv(\"test.csv\")             # Read data\n",
    "testset = testset.as_matrix()                     # Convert to ndarray\n",
    "testset = testset/255.0                             # normalize testing data\n",
    "predictions = model.predict_classes(testset)           # Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pandas.DataFrame(data=predictions, index=np.arange(1,len(predictions)+1), columns=['Label']) # Create dataframe\n",
    "submission.index.name = 'ImageId' # Set index name\n",
    "\n",
    "csv_text = submission.to_csv() # Convert to text\n",
    "\n",
    "# Write to file 'submission.csv'\n",
    "with open(\"submission.csv\",'w') as csv_file:\n",
    "    csv_file.write(csv_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27970</th>\n",
       "      <td>27971</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27971</th>\n",
       "      <td>27972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27972</th>\n",
       "      <td>27973</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27973</th>\n",
       "      <td>27974</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27974</th>\n",
       "      <td>27975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27975</th>\n",
       "      <td>27976</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27976</th>\n",
       "      <td>27977</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27977</th>\n",
       "      <td>27978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27978</th>\n",
       "      <td>27979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27979</th>\n",
       "      <td>27980</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27980</th>\n",
       "      <td>27981</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27981</th>\n",
       "      <td>27982</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27982</th>\n",
       "      <td>27983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27983</th>\n",
       "      <td>27984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27984</th>\n",
       "      <td>27985</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27985</th>\n",
       "      <td>27986</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27986</th>\n",
       "      <td>27987</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27987</th>\n",
       "      <td>27988</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27988</th>\n",
       "      <td>27989</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27989</th>\n",
       "      <td>27990</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27990</th>\n",
       "      <td>27991</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27991</th>\n",
       "      <td>27992</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27992</th>\n",
       "      <td>27993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27993</th>\n",
       "      <td>27994</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27994</th>\n",
       "      <td>27995</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      4\n",
       "4            5      3\n",
       "5            6      7\n",
       "6            7      0\n",
       "7            8      3\n",
       "8            9      0\n",
       "9           10      3\n",
       "10          11      5\n",
       "11          12      7\n",
       "12          13      4\n",
       "13          14      0\n",
       "14          15      4\n",
       "15          16      3\n",
       "16          17      3\n",
       "17          18      1\n",
       "18          19      9\n",
       "19          20      0\n",
       "20          21      9\n",
       "21          22      1\n",
       "22          23      1\n",
       "23          24      5\n",
       "24          25      7\n",
       "25          26      4\n",
       "26          27      2\n",
       "27          28      7\n",
       "28          29      9\n",
       "29          30      7\n",
       "...        ...    ...\n",
       "27970    27971      5\n",
       "27971    27972      0\n",
       "27972    27973      4\n",
       "27973    27974      8\n",
       "27974    27975      0\n",
       "27975    27976      3\n",
       "27976    27977      6\n",
       "27977    27978      0\n",
       "27978    27979      1\n",
       "27979    27980      9\n",
       "27980    27981      3\n",
       "27981    27982      1\n",
       "27982    27983      1\n",
       "27983    27984      0\n",
       "27984    27985      4\n",
       "27985    27986      5\n",
       "27986    27987      2\n",
       "27987    27988      2\n",
       "27988    27989      9\n",
       "27989    27990      6\n",
       "27990    27991      7\n",
       "27991    27992      6\n",
       "27992    27993      1\n",
       "27993    27994      9\n",
       "27994    27995      7\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission=pandas.read_csv('submission.csv')\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
